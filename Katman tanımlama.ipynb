{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "install torch",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import torch",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "torch.manual_seed(1) \n#PyTorch için rastgele sayı üreteci (random number generator) için tohum değerini ayarlar \n#Bu sayede eğitim sırasında ağırlık başlangıçları gibi rastgele işlemlerin tekrarlanabilir olmasını sağlar.\n\nx = torch.tensor([[1, 2, 3],[4,5,6]],dtype=torch.float)\ninput=x.shape[1]\n# x.shape fonksiyonu, x tensorünün boyutunu bir tuple olarak verir.\n#x.shape[1] ifadesi, x tensorünün sütun sayısını (yani 2. boyutunu) temsil eder.\n\nweight_hidden = torch.randn(input, 50,dtype=torch.float)\nbias_hidden = torch.randn(50,dtype=torch.float)\ndef tanh_activation(a):\n   return (torch.exp(a)-torch.exp(-a))/(torch.exp(a)+torch.exp(-a))\nhidden_layer = tanh_activation(torch.matmul(x,weight_hidden) + bias_hidden)\n#Bir adet hidden layer ve hidden layerda 50 nöron  var\n#Her nöron için rastgele ağırlıklarla çarptık bias değerini ekledik\n#Çıkan sonucu tanh fonksiyonunun giriş değeri yaptık\n\nweight_output = torch.randn(50,1,dtype=torch.float)\nbias_output = torch.randn(1,dtype=torch.float)\ndef sigmoid_activation(b):\n    return 1 / (1 + torch.exp(-b))\noutput_layer=sigmoid_activation(torch.matmul(hidden_layer,weight_output)+ bias_output)\n#Tek nöronlu output layerını rastgele gelen ağırlıklarla çarptık ve bias değerini ekledik\n#Çıkan sonucu sigmoid fonksiyonunun giriş değeri yaptık\n\noutput_layer",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(output_layer)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}